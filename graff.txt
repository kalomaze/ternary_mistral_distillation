och: 1, Batch: 69, Loss: 11.398945808410645, Learning Rate: 0.037200000000000004
Epoch: 1, Batch: 70, Loss: 11.335955619812012, Learning Rate: 0.037160000000000006
Epoch: 1, Batch: 71, Loss: 11.251373291015625, Learning Rate: 0.03712000000000001
Epoch: 1, Batch: 72, Loss: 11.176509857177734, Learning Rate: 0.03708
Epoch: 1, Batch: 73, Loss: 11.075740814208984, Learning Rate: 0.03704000000000001
Epoch: 1, Batch: 74, Loss: 11.054027557373047, Learning Rate: 0.037000000000000005
Epoch: 1, Batch: 75, Loss: 10.95361328125, Learning Rate: 0.03696000000000001
Epoch: 1, Batch: 76, Loss: 10.926895141601562, Learning Rate: 0.03692
Epoch: 1, Batch: 77, Loss: 10.86974811553955, Learning Rate: 0.03688
Epoch: 1, Batch: 78, Loss: 10.85558032989502, Learning Rate: 0.036840000000000005
Epoch: 1, Batch: 79, Loss: 10.814896583557129, Learning Rate: 0.036800000000000006
Epoch: 1, Batch: 80, Loss: 10.776640892028809, Learning Rate: 0.03676000000000001
Epoch: 1, Batch: 81, Loss: 10.713397026062012, Learning Rate: 0.03672
Epoch: 1, Batch: 82, Loss: 10.676926612854004, Learning Rate: 0.036680000000000004
Epoch: 1, Batch: 83, Loss: 10.657594680786133, Learning Rate: 0.036640000000000006
Epoch: 1, Batch: 84, Loss: 10.662158966064453, Learning Rate: 0.03660000000000001
Epoch: 1, Batch: 85, Loss: 10.62031078338623, Learning Rate: 0.03656
Epoch: 1, Batch: 86, Loss: 10.606749534606934, Learning Rate: 0.036520000000000004
Epoch: 1, Batch: 87, Loss: 10.602083206176758, Learning Rate: 0.036480000000000005
Epoch: 1, Batch: 88, Loss: 10.605502128601074, Learning Rate: 0.03644000000000001
Epoch: 1, Batch: 89, Loss: 10.589583396911621, Learning Rate: 0.03640000000000001
Epoch: 1, Batch: 90, Loss: 10.57269287109375, Learning Rate: 0.03636
Epoch: 1, Batch: 91, Loss: 10.565452575683594, Learning Rate: 0.036320000000000005
Epoch: 1, Batch: 92, Loss: 10.559398651123047, Learning Rate: 0.03628
Epoch: 1, Batch: 93, Loss: 10.531274795532227, Learning Rate: 0.03624000000000001
Epoch: 1, Batch: 94, Loss: 10.538742065429688, Learning Rate: 0.0362
Epoch: 1, Batch: 95, Loss: 10.521775245666504, Learning Rate: 0.036160000000000005
Epoch: 1, Batch: 96, Loss: 10.513903617858887, Learning Rate: 0.036120000000000006
Epoch: 1, Batch: 97, Loss: 10.528368949890137, Learning Rate: 0.03608
Epoch: 1, Batch: 98, Loss: 10.505123138427734, Learning Rate: 0.03604000000000001
Epoch: 1, Batch: 99, Loss: 10.508149147033691, Learning Rate: 0.036000000000000004
Epoch: 1, Batch: 100, Loss: 10.500927925109863, Learning Rate: 0.035960000000000006
Epoch: 1, Batch: 101, Loss: 10.478493690490723, Learning Rate: 0.03592
Epoch: 1, Batch: 102, Loss: 10.489341735839844, Learning Rate: 0.03588
Epoch: 1, Batch: 103, Loss: 10.484289169311523, Learning Rate: 0.035840000000000004
Epoch: 1, Batch: 104, Loss: 10.495763778686523, Learning Rate: 0.035800000000000005
Epoch: 1, Batch: 105, Loss: 10.493613243103027, Learning Rate: 0.03576000000000001
Epoch: 1, Batch: 106, Loss: 10.481133460998535, Learning Rate: 0.03572
Epoch: 1, Batch: 107, Loss: 10.469406127929688, Learning Rate: 0.03568
Epoch: 1, Batch: 108, Loss: 10.469643592834473, Learning Rate: 0.035640000000000005
Epoch: 1, Batch: 109, Loss: 10.461244583129883, Learning Rate: 0.03560000000000001
Epoch: 1, Batch: 110, Loss: 10.44843578338623, Learning Rate: 0.03556
Epoch: 1, Batch: 111, Loss: 10.464288711547852, Learning Rate: 0.03552
Epoch: 1, Batch: 112, Loss: 10.47441577911377, Learning Rate: 0.035480000000000005
Epoch: 1, Batch: 113, Loss: 10.470492362976074, Learning Rate: 0.035440000000000006
Epoch: 1, Batch: 114, Loss: 10.457724571228027, Learning Rate: 0.03540000000000001
Epoch: 1, Batch: 115, Loss: 10.481158256530762, Learning Rate: 0.03536
Epoch: 1, Batch: 116, Loss: 10.451553344726562, Learning Rate: 0.035320000000000004
Epoch: 1, Batch: 117, Loss: 10.45631217956543, Learning Rate: 0.03528
Epoch: 1, Batch: 118, Loss: 10.471766471862793, Learning Rate: 0.03524000000000001
Epoch: 1, Batch: 119, Loss: 10.445000648498535, Learning Rate: 0.0352
Epoch: 1, Batch: 120, Loss: 10.452445030212402, Learning Rate: 0.035160000000000004
Epoch: 1, Batch: 121, Loss: 10.483115196228027, Learning Rate: 0.035120000000000005
Epoch: 1, Batch: 122, Loss: 10.4437837600708, Learning Rate: 0.03508
Epoch: 1, Batch: 123, Loss: 10.459002494812012, Learning Rate: 0.03504000000000001
Epoch: 1, Batch: 124, Loss: 10.453363418579102, Learning Rate: 0.035
Epoch: 1, Batch: 125, Loss: 10.455275535583496, Learning Rate: 0.034960000000000005
Epoch: 1, Batch: 126, Loss: 10.462602615356445, Learning Rate: 0.03492
Epoch: 1, Batch: 127, Loss: 10.4506254196167, Learning Rate: 0.03488
Epoch: 1, Batch: 128, Loss: 10.454773902893066, Learning Rate: 0.03484
Epoch: 1, Batch: 129, Loss: 10.443476676940918, Learning Rate: 0.034800000000000005
Epoch: 1, Batch: 130, Loss: 10.469574928283691, Learning Rate: 0.034760000000000006
Epoch: 1, Batch: 131, Loss: 10.452794075012207, Learning Rate: 0.03472
Epoch: 1, Batch: 132, Loss: 10.44985294342041, Learning Rate: 0.03468
Epoch: 1, Batch: 133, Loss: 10.462637901306152, Learning Rate: 0.034640000000000004
Epoch: 1, Batch: 134, Loss: 10.449189186096191, Learning Rate: 0.034600000000000006
Epoch: 1, Batch: 135, Loss: 10.438260078430176, Learning Rate: 0.03456
Epoch: 1, Batch: 136, Loss: 10.427242279052734, Learning Rate: 0.03452
Epoch: 1, Batch: 137, Loss: 10.439030647277832, Learning Rate: 0.034480000000000004
Epoch: 1, Batch: 138, Loss: 10.463395118713379, Learning Rate: 0.034440000000000005
Epoch: 1, Batch: 139, Loss: 10.452302932739258, Learning Rate: 0.03440000000000001
Epoch: 1, Batch: 140, Loss: 10.449922561645508, Learning Rate: 0.03436
Epoch: 1, Batch: 141, Loss: 10.455941200256348, Learning Rate: 0.03432
Epoch: 1, Batch: 142, Loss: 10.46519947052002, Learning Rate: 0.03428
Epoch: 1, Batch: 143, Loss: 10.448617935180664, Learning Rate: 0.03424000000000001
Epoch: 1, Batch: 144, Loss: 10.444249153137207, Learning Rate: 0.0342
Epoch: 1, Batch: 145, Loss: 10.445034980773926, Learning Rate: 0.03416
Epoch: 1, Batch: 146, Loss: 10.440061569213867, Learning Rate: 0.034120000000000004
Epoch: 1, Batch: 147, Loss: 10.447412490844727, Learning Rate: 0.03408
Epoch: 1, Batch: 148, Loss: 10.45727252960205, Learning Rate: 0.03404000000000001
Epoch: 1, Batch: 149, Loss: 10.460564613342285, Learning Rate: 0.034
Epoch: 1, Batch: 150, Loss: 10.469799995422363, Learning Rate: 0.033960000000000004
Epoch: 1, Batch: 151, Loss: 10.430296897888184, Learning Rate: 0.03392
Epoch: 1, Batch: 152, Loss: 10.439627647399902, Learning Rate: 0.03388
Epoch: 1, Batch: 153, Loss: 10.439675331115723, Learning Rate: 0.03384
Epoch: 1, Batch: 154, Loss: 10.461071968078613, Learning Rate: 0.033800000000000004
Epoch: 1, Batch: 155, Loss: 10.469563484191895, Learning Rate: 0.033760000000000005
